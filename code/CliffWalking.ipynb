{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import gym # 导入gym模块\n",
    "# from envs.gridworld_env import CliffWalkingWapper # 导入自定义装饰器\n",
    "env = gym.make('CliffWalking-v0') # 定义环境\n",
    "# env = CliffWalkingWapper(env) # 装饰环境\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态数：48，动作数：4\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n # 状态数\n",
    "n_actions = env.action_space.n # 动作数\n",
    "print(f\"状态数：{n_states}，动作数：{n_actions}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始状态：36\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(f\"初始状态：{state}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2818138324.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [7]\u001B[0;36m\u001B[0m\n\u001B[0;31m    ep_reward = 0 # 记录每个回合的奖励\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# RL 代码 规范\n",
    "env = gym.make('CliffWalking-v0') # 定义环境\n",
    "# env = CliffWalkingWapper(env) # 装饰环境\n",
    "env.seed(1) # 设置随机种子\n",
    "n_states = env.observation_space.n # 状态数\n",
    "n_actions = env.action_space.n # 动作数\n",
    "agent = QLearning(n_states,n_actions,cfg) # cfg存储算法相关参数\n",
    "for i_ep in range(cfg.train_eps): # cfg.train_eps表示最大的训练回合数\n",
    "    ep_reward = 0 # 记录每个回合的奖励\n",
    "    state = env.reset() # 重置环境\n",
    "    while True:\n",
    "        action = agent.choose_action(state) # 算法选择一个动作\n",
    "        next_state, reward, done, _ = env.step(action) # 环境根据动作反馈奖励和下一个状态\n",
    "        agent.update(state, action, reward, next_state, done) # 算法更新\n",
    "        state = next_state # 更新状态\n",
    "        ep_reward += reward\n",
    "        if done: # 终止状态\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "rewards = []\n",
    "ma_rewards = [] # 滑动平均奖励\n",
    "for i_ep in range(cfg.train_eps):\n",
    "    ep_reward = 0 # 记录每个回合的奖励\n",
    "    state = env.reset() # 重置环境, 重新开始（开始一个新的回合）\n",
    "    while True:\n",
    "        action = agent.choose_action(state) # 根据算法选择一个动作\n",
    "        next_state, reward, done, _ = env.step(action) # 与环境进行一次动作交互\n",
    "        agent.update(state, action, reward, next_state, done) # Q学习算法更新\n",
    "        state = next_state # 存储上一个观察值\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(ep_reward)\n",
    "    if ma_rewards:\n",
    "        ma_rewards.append(ma_rewards[-1]*0.9+ep_reward*0.1)\n",
    "    else:\n",
    "        ma_rewards.append(ep_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q-Learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def choose_action(self, state):\n",
    "    self.sample_count += 1\n",
    "    self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                   math.exp(-1. * self.sample_count / self.epsilon_decay) # epsilon是会递减的，这里选择指数递减\n",
    "# 带有探索的贪心策略\n",
    "    if np.random.uniform(0, 1) > self.epsilon:\n",
    "        action = np.argmax(self.Q_table[ str(state)]) # 选择Q(s,a)最大值对应的动作\n",
    "    else:\n",
    "        action = np.random.choice(self.action_dim) # 随机选择动作\n",
    "    return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update(self, state, action, reward, next_state, done):\n",
    "    Q_predict = self.Q_table[ str(state)][action]\n",
    "    if done: # 终止状态\n",
    "        Q_target = reward\n",
    "    else:\n",
    "        Q_target = reward + self.gamma * np. max(self.Q_table[ str(next_state)])\n",
    "        self.Q_table[ str(state)][action] += self.lr * (Q_target - Q_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}